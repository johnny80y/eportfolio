<!DOCTYPE HTML>
<!--
	Landed by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Data Cleaning (Units 4-5)</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../assets/css/main.css" />
		<noscript><link rel="stylesheet" href="../assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">
		<div id="page-wrapper">

			<!-- Header -->
				<header id="header">
					<h1 id="logo"><a href="Landing_Page_Module_3.html">back</a></h1>
					<nav id="nav">
						<ul>
							<li><a href="index.html">home</a></li>
							
							<li><a href="../contact.html">Get in Touch</a></li>
						</ul>
					</nav>
				</header>

			<!-- Main -->
				<div id="main" class="wrapper style1">
					<div class="container">
						<header class="major">
							<h2>Data Cleaning (Units 4-6)</h2>
							<p>Deciphering Big Data</p>
						</header>

						<!-- Content -->
							<section id="content">
								<a href="#" class="image fit"><img src="images/ink_cropped.jpg" alt="" /></a> <!-- original image: pic07.jpg -->
								<h3>Generator Expressions (Week 4)</h3>
								<p>
									<p>This week, I came into contact with the concept of a generator expression for the first time. Generator expressions in Python are a powerful tool for creating and working with sequences of values. They are similar to list comprehensions, but instead of creating a list, they generate the values on-the-fly as they are needed. This makes them more memory efficient, as they only store one value at a time rather than an entire list in memory (PythonTutorial, 2023).</p>
									<p>Generator expressions can be used in a variety of scenarios, such as when you need to iterate over a large dataset or when you need to perform calculations on a sequence of values without storing them all in memory. They can also be used in conjunction with other functions like filter() and map() to perform complex operations on data.</p>
									<p>One advantage of generator expressions over list comprehensions is their lazy evaluation (Sarkar & Roychowdhury, 2019). Since they generate values on-the-fly, they only compute the values that are actually needed, which can significantly reduce the amount of memory required. I imagine that this will make generator expressions especially useful in the context of Big Data.</p>
									<p>Overall, generator expressions are a powerful tool in the Python language and one that will definitely be useful for future projects I may wish to engage in. Instead of loading an entire dataset into memory, it is possible to go through a dataset item-by-item and thus save time and memory. This property of generator expressions is also a downside, however. Being limited to item-by-item iterations (Strika, 2018) limits the possible operations that can be performed on a dataset as a whole or slices of it.</p>
																		
								</p>
								
								<b> Sources </b>
								<ul>
								<li> PythonTutorial (2023) Python Generator Expressions. Available from: https://www.pythontutorial.net/advanced-python/python-generator-expressions/ [Accessed 18 February 2023].</li>
								<li>Sarkar, T. & Roychowdhury, S. (2019) Data Wrangling with Python. 1st ed. Birmingham: Packt Publishing Ltd.</li>
								<li>Strika, L. (2018) Python’s Generator Expressions: Fitting Large Datasets into Memory. Available from: https://towardsdatascience.com/pythons-list-generators-what-when-how-and-why-2a560abd3879 [Accessed 18 February 2023].</li>
								</ul>
								
								<!-- Next Entry -->
								<br>
								<h3>Data Wrangling Exercise (Week 5)</h3>
								<p>
									<p>Data wrangling is an essential process in data science that involves cleaning, transforming, and preparing data for analysis. Python is a powerful language for data wrangling, and I have used it as an effective tool in my own projects. This week, we were supposed to conduct a data wrangling exercise but as the exercise was inaccessible to me at that time, I worked on a different dataset and performed my own small analysis.</p>
									<p>Using data from the GitHub repository linked in the module’s workspace (Kazil 2014), I loaded two dataframes into a jupyter notebook. One contained UNICEF fertility rates and the other contained HIV/AIDS infection rates (ibid.). </p>
									<pre>
										<code>
# Code for loading the two dataframes:
# Fertility rates:
url = 'https://raw.githubusercontent.com/jackiekazil/data-wrangling/master/data/unicef/fertility_rate.csv'
fert = pd.read_csv(url, skiprows=2)
# HIV rates:
hiv = pd.read_excel('/content/drive/My Drive/Colab Notebooks/Deciphering Big Data/Week 5 exercize/hiv_aids_2014.xlsx', sheet_name = 'T3_AdolescentEpi', skiprows=5)
										</code>
									</pre>
									
									<p>The wrangling tasks I performed included renaming columns. In Python, this can be easily done using the rename() method. It is important to choose clear and informative column names to facilitate analysis and interpretation.</p>
									<pre>
										<code>
hiv.rename(columns = {'Total':'Population2014_thousands'}, inplace = True)
hiv.rename(columns = {'Age 10-19':'Adolescents2014_thousands'}, inplace = True)
										</code>
									</pre>

									<p>Subsetting data in Python is important because it allows for focusing on specific portions of a dataset that are relevant to a particular analysis. By selecting only the necessary data, the analysis is more efficient (meaning I get less confused).</p>
									<pre>
										<code>
# Subsetting the data:
hiv = hiv[['country', 'Population2014_thousands', 'Adolescents2014_thousands', 'Adolescents_with_HIV', 'Population_Percentage_Adolescents']]
										</code>
									</pre>

									<p>Another useful data wrangling technique is removing substrings from column values. This can be achieved with the str.replace() method. It is particularly helpful when dealing with messy data that contains unnecessary characters or strings.</p>
									<pre>
										<code>
# Clean the Adolescents_with_HIV-column (drop < and >):
hiv["Adolescents_with_HIV"] = hiv["Adolescents_with_HIV"].str.replace("<","")
hiv["Adolescents_with_HIV"] = hiv["Adolescents_with_HIV"].str.replace(",","")
# Clean the data by removing "-" values:
hiv = hiv.replace('–', np.nan)
										</code>
									</pre>

									<p>Changing the datatype of columns is also an important step in data wrangling. This is accomplished using the astype() method. It is crucial to ensure that the data types are appropriate for the analysis being conducted to avoid errors and inaccuracies.</p>
									<pre>
										<code>df['Population_Percentage_Adolescents'] = df['Population_Percentage_Adolescents'].astype(float)</code>
									</pre>

									<p>Finally, merging two dataframes is a common operation in data wrangling. This is done using the merge() method. It combines data from different sources to create a unified dataset for analysis.</p>
									<pre>
										<code>
# Merge the two dataframes by country:
df = fert.merge(right=hiv, 
				how='inner', # inner join to only keep data for countries present in both tables
				left_on = 'Country Name',
				right_on = 'country')
										</code>
									</pre>

									<p>In conclusion, data wrangling is a critical process in data science, and Python is an effective tool for accomplishing it. Renaming columns, removing substrings, changing datatypes, and merging dataframes are just a few examples of the techniques that can be used to prepare data for analysis. </p>
									<p>As this entire project would have been uninteresting without performing some basic analyses with the freshly cleaned and merged data, here are some of the insights I won. First, I wondered which countries had the highest adolescent fertility rates (in 2000) and the highest percentage of adolescents (in 2014).</p>
									<pre>
										<code>
df.nlargest(5, ['2000'])
# Results: Niger, Chad, Angola, Mali, Uganda
df.nlargest(5, ['Population_Percentage_Adolescents'])
# Results: Timor-Leste, Afghanistan, Ethiopia, Uganda, Chad
										</code>
									</pre>

									<p>Next, I wondered whether there was a connection between the fertility rate among adolescents in 2000 and the number of adolescents in 2014. The hypothesis is that more youth pregnancies lead to a higher number of young people 14 years later. The data seems to support this.</p>
									<pre>
										<code>
sns.regplot(df['2000'], df['Population_Percentage_Adolescents'], 
			ci=95, # 95% Confidence Interval
			order = 2, # Add a polinomial of the power 2 (a simple linear regression clearly doesn't fit the data well)
			).set(title='Impact of Adolescent Fertility in 2000 on the Number of Adolescents in 2014',
					xlabel = 'Adolescent Fertility Rate in 2000 (per 1.000 women)',
					ylabel = 'Percentage of Adolescents in 2014')
										</code>
									</pre>

									<p>
										<img src="../images/week5_graph1.png" alt="Graph 1" style="width7260px;height:360px;background-color:grey">
									</p>

									<p>Finally, I looked at the connection between adolescent fertility rates and adolescent HIV/AIDS infections. The hypothesis is that higher fertility rates imply more sexual activity and this would increase HIV/AIDS infections. The data did not, however, support this conclusion (the 95% confidence interval encompasses nearly the entire effect range).</p>
									<pre>
										<code>
df['Adolescents_with_HIV'] = df['Adolescents_with_HIV'].astype(float)
sns.regplot(df['2012'], df['Adolescents_with_HIV'], 
			ci=95, # 95% Confidence Interval
			order = 2, # Add a polinomial of the power 2 (a simple linear regression clearly doesn't fit the data well)
			).set(title='Impact of Adolescent Fertility in 2012 on the Number of Adolescents with HIV in 2014',
					xlabel = 'Adolescent Fertility Rate in 2012 (per 1.000 women)',
					ylabel = '% of Adolescents with HIV in 2014')
										</code>
									</pre>

									<p>
										<img src="../images/week5_graph2.png" alt="Graph 2" style="width7260px;height:360px;background-color:grey">
									</p>

								<b> Sources </b>
								<ul>
								<li>Kazil, J. (2014) Data Wrangling. Available from: https://github.com/jackiekazil/data-wrangling/tree/master/data/unicef [Accessed 20 February 2023].</li>

								</ul>
								
								<!-- Next Entry -->
								<br>
								<h3>Final Data Wrangling Exercise (Week 6)</h3>
								<p>
									<p>This week consisted mostly of a large Data Wrangling exercise using two datasets from the United Nations (Packt, 2019) and the World Bank Group (ibid.). The task was described as follows: “In India, did the enrollment in primary/secondary/tertiary education increase with the improvement of per capita GDP in the past 15 years?” (Sarkar & Roychowdhury, 2019).</p>
									<p>As last week’s exercise already included many data wrangling tasks such as subsetting and merging dataframes, this ePortfolio post will focus on another topic. The main skill I learned from this exercise was linear data imputation. “Interpolation is a technique in Python used to estimate unknown data points between two known data points.” (Chanda, 2022). Basically, imputation allows analysts to make use of observations that are incomplete or missing entirely. </p>
									<p>The first step to filling gaps in the data is to identify such gaps. I had subset my data into three separate dataframes. Each contained UN data about Indian school enrollments but for different education levels (primary, secondary, and tertiary). The problem was that several years contained no reported data.</p>
									<pre>
										<code>
# Which Years are Missing for India:
india_primary = un_data[(un_data['Region/Country/Area'] == 'India') &
						(un_data['Data'] == 'Students enrolled in primary education (thousands)')]
india_secondary = un_data[(un_data['Region/Country/Area'] == 'India') &
						(un_data['Data'] == 'Students enrolled in secondary education (thousands)')]
india_tertiary = un_data[(un_data['Region/Country/Area'] == 'India') &
						(un_data['Data'] == 'Students enrolled in tertiary education (thousands)')]

print(india_primary.Year) # We are missing 2004:2009 and 2011:2013
										</code>
									</pre>

									<p>The next step consisted of creating a list of the missing years as well as lists for the other columns in the original dataframe (area, data, value, and footnotes).</p>
									<pre>
										<code>
filler_years = [y for y in range(2004,2010)]+[y for y in range(2011,2014)]
filler_area = ['India' for y in range(1, 10)]
filler_data = [np.nan for y in range(1, 10)]
filler_value = [np.nan for y in range(1, 10)]
filler_footnotes = [np.nan for y in range(1, 10)]
print(filler_value)
										</code>
									</pre>

									<p>Next, I combine all of these lists into a dataframe that contains the same columns as the original UN data but only includes the years that were missing.</p>
									<pre>
										<code>
filler_data = pd.DataFrame(list(zip(filler_area, filler_years, filler_data, filler_value, filler_footnotes)),
			columns =['Region/Country/Area', 'Year', 'Data', 'Value', 'Footnotes'])
filler_data.head(2)
										</code>
									</pre>

									<p>I merge this new dataframe with the original UN data, sort by year, and reset the index.</p>
									<pre>
										<code>
# Merge this with Original Dataframe:
india_primary = india_primary.append(filler_data, sort=True)
india_secondary = india_primary.append(filler_data, sort=True)
india_tertiary = india_primary.append(filler_data, sort=True)

# Sort by Year and Reset Index:
india_primary.sort_values(by='Year', inplace=True, ignore_index = True)
india_secondary.sort_values(by='Year', inplace=True, ignore_index = True)
india_tertiary.sort_values(by='Year', inplace=True, ignore_index = True)
india_tertiary.Year
										</code>
									</pre>

									<p>Finally, this is where the actual data imputation takes place. Using the interpolate method provided by the Pandas library (Pandas, 2023). In simple terms, linear interpolation estimates the missing value to fall on a line between the previous and the following observations (Chanda, 2022).</p>
									<pre>
										<code>
# Fill the Missing Values in the "Value"-Column with Lineary Interpolated Data:
india_primary['Value'].interpolate(inplace=True, method ='linear')
india_secondary['Value'].interpolate(inplace=True, method ='linear')
india_tertiary['Value'].interpolate(inplace=True, method ='linear')
india_tertiary.head()
										</code>
									</pre>

									<p>In conclusion, Interpolation allows analysts to make use of data that contains missing observations by linearly estimating reasonable values. I would imagine that interpolation works well for columns that are increasing or decreasing monotonously. However, for instances of high volatility (e.g. crypto currency prices), interpolation will likely lead to faulty data. I would love to further explore the implications of interpolation further in the upcoming units.</p>
									
								</p>
								<b>Sources</b>
								<ul>
									<li>Chanda, J. (2022) Interpolation using pandas. Available from: https://www.numpyninja.com/post/interpolation-using-pandas [Accessed 27 February 2023].</li>
									<li>Packt (2019) Data Wrangling with Python. Lesson 09. Activity 12-15. Available from: https://github.com/TrainingByPackt/Data-Wrangling-with-Python/tree/master/Lesson09/Activity12-15 [Accessed 22 February 2023].</li>
									<li>Pandas (2023) pandas.Series.interpolate. Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html [Accessed 27 February 2023].</li>
									<li>Sarkar, T. & Roychowdhury, S. (2019) Data Wrangling with Python. 1st ed. Birmingham: Packt Publishing Ltd.</li>
								</ul>


							</section>

					</div>
				</div>

			<!-- Footer -->
				<footer id="footer">
					<ul class="icons">
						<li><a href="https://twitter.com/RealJohnGeiger" class="icon brands alt fa-twitter"><span class="label">Twitter</span></a></li>
						<li><a href="https://de.linkedin.com/in/johannes-geiger?trk=people-guest_people_search-card" class="icon brands alt fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
						<li><a href="https://www.instagram.com/realjohngeiger/" class="icon brands alt fa-instagram"><span class="label">Instagram</span></a></li>
						<li><a href="https://github.com/johnny80y/" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
					</ul>
					<ul class="copyright">
						<li>&copy; Untitled. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>
				</footer>

		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.dropotron.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
