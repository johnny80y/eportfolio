<!DOCTYPE HTML>
<!--
	Landed by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Technologies (Units 9-11)</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../assets/css/main.css" />
		<noscript><link rel="stylesheet" href="../assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">
		<div id="page-wrapper">

			<!-- Header -->
				<header id="header">
					<h1 id="logo"><a href="Landing_Page_Module_3.html">back</a></h1>
					<nav id="nav">
						<ul>
							<li><a href="index.html">home</a></li>
							
							<li><a href="../contact.html">Get in Touch</a></li>
						</ul>
					</nav>
				</header>

			<!-- Main -->
				<div id="main" class="wrapper style1">
					<div class="container" align="justify">
						<header class="major">
							<h2>Technologies (Units 9-11)</h2>
							<p>Deciphering Big Data</p>
						</header>



								
<!-- Entry 1--><br>
								<p>
									<h3>Database Building Exercise</h3>
									(11 March 2023)
								</p>	
								<p>
									This week we were tasked with building our first database in the Python library SQLite. As I am a visual learner, my first step was to watch an SQLite tutorial on YouTube (Schafer, 2017). There, I learned the basic commands for setting up a database in SQLite, filling in data, and submitting queries.
								</p>
								<p>
									The exercise for the week instructed us to fill a database with the UNICEF data from week 4. However, I opted to set up a relational database that is roughly based on last week’s normalization task. I thought this would make for a more interesting example as I can add additional tables and relationships to increase the database’s complexity. The database I will try to implement looks like this:
								</p>
							</p>
							<center>
								<img src="../images/2023_03_10_DB-Diagram_Week9.png" alt="Graph 2" style="height:360px">
								<br>The database diagram was created using QuickDBD (2023).
							</center>
							<br><br>
							<p>
								The first step to implementing this database in SQLite was to set up an empty database in a Jupyter notebook. To facilitate easier modifications, I opted to store the database in memory rather than in a file. This means that I can simply rerun the following command and have a clean database in case I make any mistakes. In hindsight, this saved me a lot of time. The downside is that I cannot fill a purely in-memory database with large amounts of data. For this exercise, however, that does not matter.
							</p>
							<pre>
								<code>
import sqlite3
import pandas as pd

# Create an empty database in memory:
conn = sqlite3.connect(':memory:')
								</code>
							</pre>
							<p>
								Next, I create all the tables and relationships. I only show the code for the first two tables because the syntax is almost identical for all tables:
							</p>
							<pre>
								<code>
# Create the Students table:
conn.execute('''
CREATE TABLE Students (
    StudentNumber INTEGER PRIMARY KEY NOT NULL,
    StudentName TEXT  NOT NULL,
    GPA INTEGER,
    Support INTEGER,
    Birthday TEXT
 );''')

# Create the Courses table:
conn.execute('''
CREATE TABLE Courses(
    CourseName TEXT PRIMARY KEY NOT NULL,
    TeacherName TEXT NOT NULL,
    Department TEXT NOT NULL,
    FOREIGN KEY(TeacherName) REFERENCES Teachers(TeacherName),
    FOREIGN KEY(Department) REFERENCES Departments(Department)
);''')
								</code>
							</pre>
							<p>
								The first thing I learned is that SQLite uses different data types than MySQL. In SQLite, there are only five options – INTEGER, TEXT, NULL, REAL, and BLOB (SQLite, 2022a). This means that I had to modify the datatypes I had originally planned to use. VARCHAR, for example, had to become TEXT and BOOL had to become INTEGER. This limitation in available data types, among other factors, makes SQLite an inefficient choice for production environments with large amounts of data or high-volume data streams (SQLite 2022b). However, given its extremely simple setup, it is likely a great choice for the prototyping stage of the database system development lifecycle, as described by Connolly and Begg (2014).
							</p>
							<p>
								After creating all tables in my database, I commit the changes. To check my work, I ask Python to show me all tables in the database:
							</p>
							<pre>
								<code>
# Commit changes to DB:
conn.commit()

# Create cursor object:
cursor = conn.cursor()

# Show the tables:
cursor.execute("""SELECT name FROM sqlite_master WHERE type='table';""")
cursor.fetchall()

##### OUTPUT: #####
[('Students',),
 ('Courses',),
 ('Enrollments',),
 ('Teachers',),
 ('Departments',)]
								</code>
							</pre>
							<p>
								Using the PRAGMA extension, which enables users to query an SQLite database for internal non-table data (SQLite, 2023), I display the setup of my tables. For the Students table this looks as follows:
							</p>
							<pre>
								<code>
cursor.execute("PRAGMA table_info('Students')").fetchall()
##### OUTPUT: #####
[(0, 'StudentNumber', 'INTEGER', 1, None, 1),
 (1, 'StudentName', 'TEXT', 1, None, 0),
 (2, 'GPA', 'INTEGER', 0, None, 0),
 (3, 'Support', 'INTEGER', 0, None, 0),
 (4, 'Birthday', 'TEXT', 0, None, 0)]
								</code>
							</pre>
							<p>
								I find that the database and all tables have been set up correctly. This means that I now start filling in sample data. The sample data was created in Excel beforehand, so I could load it using Pandas and then fill the database table by table. This seemed like a more realistic way of setting up and populating a database compared with adding individual data entities one at a time.
							</p>
							<p>
								One after another, I filled the tables with data. What I learned in doing so is that I had to start from those tables that did not have foreign keys within them. Columns for which I specified the “NOT NULL” option, can not be filled with data if they contain a foreign key and the referenced item in the foreign table does not yet exist.
							</p>
							<p>
								The example code for filling the Students table is shown in the following code block. I used the .to_sql function (Pandas, 2023) to add append the contents from a pandas dataframe called “students_data” to the SQLite table called “Students”:
							</p>
							<pre>
								<code>
# Fill the Students table:
students_data.to_sql('Students', conn, if_exists='append')

# Check if it worked:
cursor.execute("SELECT * FROM Students")
cursor.fetchall()

##### Output: #####
[(1, 'Sam Smith', 2, 1, '3 June 1992'),
 (2, 'Bob Baker', 3, 1, '2 May 1995'),
 (3, 'Anne Alison', 1, 0, '18 January 1993'),
 (4, 'Jamie Johnson', 1, 0, '12 December 2000'),
 (5, 'Mark Meadows', 2, 1, '30 August 1992\n')]
								</code>
							</pre>
							<p>
								After repeating this step for all other tables, working up the tree of relationships to avoid empty foreign key links, my database was completed. In order to check if all relationships worked the way they should, I performed a number of queries using JOINs. This way, I could verify that all relationships were implemented correctly and I could correctly reference data entities across different tables. Two of these sample queries are shown below.
							</p>
							<p>
								The first sample query generates a table with all course names, the responsible teachers, as well as the teachers’ office numbers, departments, and bosses:
							</p>
							<pre>
								<code>
# Sample Query getting a table with all course names, the responsible teacher, 
# their office number, their department, and their head of department:
cursor.execute('''
  SELECT 
    c.CourseName,
    t.TeacherName,
    t.OfficeNumber,
    d.Department,
    d.HeadOfDepartment
  FROM Courses AS c
  INNER JOIN Teachers AS t
    ON c.TeacherName = t.TeacherName
  INNER JOIN Departments as d
    ON c.Department = d.Department
''')
cursor.fetchall()

##### Output: #####
[('Mathematics', 'Sam Smith', 13, 'Natural Science', 'Nick Nicolls'),
 ('Biology', 'Ronald Roth', 45, 'Natural Science', 'Nick Nicolls'),
 ('English', 'Paula Penn', 223, 'Arts and Humanities', 'Ariadna Abott'),
 ('Geography', 'Regina Rich', 312, 'Natural Science', 'Nick Nicolls'),
 ('Art', 'Beth Botts', 8, 'Arts and Humanities', 'Ariadna Abott')]
								</code>
							</pre>
							<p>
								The second sample query creates a table of all students, their classes, and their teachers:
							</p>
							<pre>
								<code>
# Sample query creating a table of all students, the courses they're enrolled into,
# and the teachers who teach those courses:
cursor.execute('''
  SELECT
    s.StudentName,
    c.CourseName,
    c.TeacherName
  FROM Enrollments AS e
  INNER JOIN Students AS s
    ON e.StudentNumber = s.StudentNumber
  INNER JOIN Courses AS c
    ON c.CourseName = e.CourseName
''')
cursor.fetchall()

##### Output: #####
[('Sam Smith', 'Mathematics', 'Sam Smith'),
 ('Sam Smith', 'Biology', 'Ronald Roth'),
 ('Sam Smith', 'Geography', 'Regina Rich'),
 ('Sam Smith', 'Art', 'Beth Botts'),
 ('Bob Baker', 'Mathematics', 'Sam Smith'),
 ('Bob Baker', 'Biology', 'Ronald Roth'),
 ('Bob Baker', 'English', 'Paula Penn'),
 ('Bob Baker', 'Geography', 'Regina Rich'),
 ('Bob Baker', 'Art', 'Beth Botts'),
 ('Anne Alison', 'English', 'Paula Penn'),
 ('Anne Alison', 'Geography', 'Regina Rich'),
 ('Anne Alison', 'Art', 'Beth Botts'),
 ('Anne Alison', 'History', 'Gerald Green'),
 ('Jamie Johnson', 'Mathematics', 'Sam Smith'),
 ('Jamie Johnson', 'English', 'Paula Penn'),
 ('Jamie Johnson', 'Geography', 'Regina Rich'),
 ('Jamie Johnson', 'Art', 'Beth Botts'),
 ('Jamie Johnson', 'History', 'Gerald Green'),
 ('Mark Meadows', 'Mathematics', 'Sam Smith'),
 ('Mark Meadows', 'Art', 'Beth Botts')]
								</code>
							</pre>
							<p>
								Everything worked well and I am pleased to have implemented my first database in SQLite. I learned that SQLite is great for small, integrated database setups but not for large-scale production environments. Further, I originally struggled to fill my tables with data because I had not satisfied all foreign key requirements. Only after encountering unmet requirements did I go back to the database diagram and specifically plan out an order in which the tables would need to be filled.
							</p>
								

								<p>
									<b>Sources</b>
									<ul>
										<li>
											Connolly, T., Begg, C. (2014) <i>Database Systems. A Practical Approach to Design, Implementation, and Management</i>. 6th ed. Harlow: Pearson Education Limited.
										</li>
										<li>
											Pandas (2023) ppandas.DataFrame.to_sql. Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html [Accessed 04 March 2023].
										</li>
										<li>
											QuickDBD (2023) QuickDBD. Available from: https://app.quickdatabasediagrams.com/#/ [Accessed 11 March 2023].
										</li>
										<li>
											Schafer, C. (2017) Python SQLite Tutorial: Complete Overview - Creating a Database, Table, and Running Queries. Available from: https://www.youtube.com/watch?v=pd-0G0MigUA&t=1453s [Accessed 11 March 2023].
										</li>
										<li>
											SQLite (2022a) Datatypes in SQLite. Available from: https://www.sqlite.org/datatype3.html [Accessed 12 March 2023].
										</li>
										<li>
											SQLite (2022b) Appropriate Uses for SQLite. Available from: https://www.sqlite.org/whentouse.html [Accessed 12 March 2023].
										</li>
										<li>
											SQLite (2023) PRAGMA Statements. Available from: https://www.sqlite.org/pragma.html [Accessed 12 March 2023].
										</li>
									</ul>
								</p>



<!-- Entry 2--><br>
								<p>
									<h3>Implementing Our Group Project’s Database in SQLite</h3>
									(12 March 2023)
								</p>	

								<p>
									As our instructor recommended that we should set up a prototype database with SQLite for our group project, I decided to use the skills we learned in the data normalization task to create the necessary database as an exercise. As I am doing this ahead of schedule, there may be an update later on, once my group partner has a chance to look at my code. She will surely contribute a number of helpful improvements and/or spot errors I have made. If so, I will post an updated version of the code later on.
								</p>
								<p>
									The database we designed was for a fictional food delivery startup called “FoodTaxi”, which is closely modeled on UBER Eats. The relational database was modeled using QuickDBD (2023) and looks like this:
								</p>
								<center>
									<img src="../images/group_project_DBD.png" alt="Graph 2" style="height:700px">
								</center>
								<br>
								<p>
									The first step is to load the libraries we will need and to set up a SQL database. I opt to store the database in memory, though storing it in a file would be just as easy. The reason why storing it in memory is a good option for a prototype like this is because it is easier to overwrite/change and because the RAM requirements of a small database holding very few sample entries is almost negligible.
								</p>
								<pre>
									<code>
import sqlite3
import pandas as pd

# Create an empty database in memory 
# (this makes it easier to reset it if mistakes are made or tests mess it up):
conn = sqlite3.connect(':memory:')

# Create a cursor object
cursor = conn.cursor()
									</code>
								</pre>
								<p>
									The next step is to create the various tables and define the data type for each attribute. As mentioned previously, SQLite only supports a very limited selection of data types, so some simplifications must be made. The following sample code shows how I set up the table for customers. The setup for all other tables follows analogously:
								</p>
								<pre>
									<code>
# Create the Customer table.
# Note that the AUTOINCREMENT keyword is not needed in SQLite as it 
# is automatically applied to the PK.
cursor.execute('''
  CREATE TABLE Customer(
    CustomerID    INTEGER     PRIMARY KEY     NOT NULL , 
    FirstName     TEXT        NOT NULL ,
    Surname       TEXT        NOT NULL ,
    TownOrCity    TEXT        NOT NULL ,
    County        TEXT        NOT NULL ,
    PostalCode    TEXT        NOT NULL ,
    Street        TEXT        NOT NULL ,
    StreetNo      INTEGER     NOT NULL ,
    Phone         INTEGER     NOT NULL ,
    PaymentID     INTEGER     NOT NULL ,
    FOREIGN KEY(PaymentID)    REFERENCES    PaymentInfo(PaymentID)
);''')

# Show the table setup:
cursor.execute("PRAGMA table_info('Customer')").fetchall()

##### Output: #####
[(0, 'CustomerID', 'INTEGER', 1, None, 1),
 (1, 'FirstName', 'TEXT', 1, None, 0),
 (2, 'Surname', 'TEXT', 1, None, 0),
 (3, 'TownOrCity', 'TEXT', 1, None, 0),
 (4, 'County', 'TEXT', 1, None, 0),
 (5, 'PostalCode', 'TEXT', 1, None, 0),
 (6, 'Street', 'TEXT', 1, None, 0),
 (7, 'StreetNo', 'INTEGER', 1, None, 0),
 (8, 'Phone', 'INTEGER', 1, None, 0),
 (9, 'PaymentID', 'INTEGER', 1, None, 0)]
									</code>
								</pre>
								<p>
									The most complicated tables are the Orders table and the ShoppingCart table. Both require a large number of linkages to other tables and the ShoppingCart table is the only one using a compound primary key. The code for these two tables looks as follows:
								</p>
								<pre>
									<code>
# Create the Orders table:
cursor.execute('''
  CREATE TABLE Orders(
    OrderID       INTEGER     PRIMARY KEY    NOT NULL ,
    CustomerID    INTEGER     NOT NULL ,
    RestaurantID  INTEGER     NOT NULL ,
    DeliveryID    INTEGER     NOT NULL ,
    OrderStatus   INTEGER     NOT NULL ,
    DriverID      INTEGER     NOT NULL ,
    OrderTime     TEXT        NOT NULL ,
    FOREIGN KEY(CustomerID)   REFERENCES    Customer(CustomerID) ,
    FOREIGN KEY(RestaurantID) REFERENCES    Restaurant(RestaurantID) ,
    FOREIGN KEY(DeliveryID)   REFERENCES    DeliveryAddress(DeliveryID) ,
    FOREIGN KEY(DriverID)     REFERENCES    Driver(DriverID)
);''')

# Create the Shopping Cart table:
cursor.execute('''
  CREATE TABLE ShoppingCart(
    OrderID           INTEGER    NOT NULL ,
    FoodID            INTEGER    NOT NULL ,
    Quantity          INTEGER    NOT NULL ,
    FOREIGN KEY(OrderID)  REFERENCES    Orders(OrderID) ,
    FOREIGN KEY(FoodID)   REFERENCES    Food(FoodID),
    PRIMARY KEY(OrderID, FoodID)
);''')
# Notw how the compound primary key needs to be set differently than in other
# tables with a simple ID-column.
									</code>
								</pre>
								<p>
									After all tables were successfully created, I commit the changes to my database object and display all tables in the database:
								</p>
								<pre>
									<code>
# Commit changes to DB:
conn.commit()

# Show the list of tables:
cursor.execute("""SELECT name FROM sqlite_master WHERE type='table';""")
cursor.fetchall()

##### Output: #####
[('Customer',),
 ('Restaurant',),
 ('Driver',),
 ('Food',),
 ('Orders',),
 ('PaymentInfo',),
 ('ShoppingCart',),
 ('MenuCategory',),
 ('DeliveryAddress',)]
									</code>
								</pre>
								<p>
									To test our prototype database, I load in a number of .csv files which I manually filled with customers, restaurants, drivers, food items, orders, etc. I stored these on GoogleDrive, so I mount my drive and load them as Pandas dataframes:
								</p>
								<pre>
									<code>
# Load sample data from GoogleDrive:
from google.colab import drive
drive.mount('/content/drive')

df_cust = pd.read_csv('/filepath/Customer.csv')
df_rest = pd.read_csv('/filepath/Restaurant.csv')
# ......the code is identical for all other tables.
									</code>
								</pre>
								<p>
									Next, because a Pandas dataframe uses an index and this would be recognized as a column by SQLite, I must reset all indices to the respective primary key columns. As the primary key for each table is also unique, it can be used as our dataframe index.
								</p>
								<p>
									The only problem is the ShoppingCart-table. As it uses a compound primary key, I cannot use any of its columns as a dataframe index individually. I will fix this problem at a later point when populating my database from this dataframe. For all dataframes except the ShoppingCart, I reset the index as follows:
								</p>
								<pre>
									<code>
# Reset indices to primary keys :
df_cust.set_index('CustomerID', inplace=True)
df_rest.set_index('RestaurantID', inplace=True)
# ......the code is identical for all other tables.
									</code>
								</pre>
								<p>
									We are now ready to fill our SQLite tables with the sample data. We do this using the dataframe.to_sql method (Pandas, 2023):
								</p>
								<pre>
									<code>
# Fill the Customer table:
df_cust.to_sql('Customer', conn, if_exists='append')
# ......the code is identical for all other tables.
									</code>
								</pre>
								<p>
									The ShoppingCart table is an exception as it uses a compound primary key. Therefore, I must make sure that the dataframe.to_sql method ignores the Pandas index and does not see it as a data column. This can be achieved by setting the index option to “False”.
								</p>
								<pre>
									<code>
# Fill the ShoppingCart table:
df_shopping.to_sql('ShoppingCart', conn, if_exists='append', index=False) 
									</code>
								</pre>
								<p>
									After filling in all of the sample data, the prototype database is finished and ready for testing. As above, we can run a few sample queries using JOIN operations to see if all tables have been linked correctly. For the first example, I query all items that have been ordered and the quanitity in which they were ordered. I purposefully pull in data from more tables than is strictly necessary because I wish to test whether the relationships between my tables function correctly.
								</p>
								<pre>
									<code>
# Query all the items ordered and their quantity:
cursor.execute('''
  SELECT
    s.OrderID,
    o.CustomerID,
    f.FoodName,
    s.Quantity
  FROM ShoppingCart AS s
  
  INNER JOIN Orders AS o
    ON o.OrderID = s.OrderID

  INNER JOIN Food as f
    ON f.FoodID = s.FoodID
''')
cursor.fetchall()

##### Output: #####
[(1, 1, 'Pizza Margherita', 2),
 (1, 1, 'Gulab Jamun', 1),
 (1, 1, 'Ice cream', 3),
 (2, 2, 'White Sausages', 2),
 (2, 2, 'Curry', 3),
 (2, 2, 'Beer', 3),
 (3, 3, 'Vienna Sausages', 1),
 (3, 3, 'White Sausages', 1),
 (3, 3, 'Curry', 2)]
									</code>
								</pre>
								<p>
									As this worked well, I try to get a list of all items ordered by the user named “Susan Holmes”. This required me to link the Customer table with the ShoppingCart table. These tables are not directly connected, so I had to indirectly link them using the Orders table as an intermediary. To learn about indirect JOIN operations, I referred to a tutorial (SQLiteTutorials 2022).
								</p>
								<pre>
									<code>
# Query all the items ordered by Susan Holmes:
cursor.execute('''
  SELECT 
    c.Surname,
    f.FoodName,
    s.Quantity
  FROM ShoppingCart AS s

  INNER JOIN Orders AS o       ON o.OrderID = s.OrderID
  
  INNER JOIN Customer AS c     ON o.CustomerID = c.CustomerID
  
  INNER JOIN Food AS f         ON f.FoodID = s.FoodID

  WHERE c.Surname = 'Holmes'

''')
cursor.fetchall()


##### Output: #####
[('Holmes', 'Pizza Margherita', 2),
 ('Holmes', 'Gulab Jamun', 1),
 ('Holmes', 'Ice cream', 3)]
									</code>
								</pre>
								<p>
									To check if the Drivers table was implemented correctly, I queried all items delivered by the driver named “Fernando Alonzo”:
								</p>
								<pre>
									<code>
# Query all items delivered by Fernando Alonzo:
cursor.execute('''
  SELECT 
    d.Surname,
    f.FoodName,
    s.Quantity
  FROM ShoppingCart AS s

  INNER JOIN Orders AS o       ON o.OrderID = s.OrderID
  
  INNER JOIN Driver AS d     ON o.DriverID = d.DriverID
  
  INNER JOIN Food AS f         ON f.FoodID = s.FoodID

  WHERE d.Surname = 'Alonzo'

''')
cursor.fetchall()


##### Output: #####
[('Alonzo', 'White Sausages', 2),
 ('Alonzo', 'Curry', 3),
 ('Alonzo', 'Beer', 3)]
									</code>
								</pre>
								<p>
									Finally, I perform a query that outputs all order items that were processed by the restaurant called “Hans’s Sausages”:
								</p>
								<pre>
									<code>
# Query all orderes processed by Hans's Sausages:
cursor.execute('''
  SELECT 
    r.RestaurantName,
    f.FoodName,
    s.Quantity
  FROM ShoppingCart AS s

  INNER JOIN Orders AS o       ON o.OrderID = s.OrderID
  
  INNER JOIN Restaurant AS r     ON o.RestaurantID = r.RestaurantID
  
  INNER JOIN Food AS f         ON f.FoodID = s.FoodID

  WHERE r.RestaurantName = "Hans's Sausages"

''')
cursor.fetchall()

##### Output: #####
[("Hans's Sausages", 'Vienna Sausages', 1),
 ("Hans's Sausages", 'White Sausages', 1),
 ("Hans's Sausages", 'Curry', 2)]
									</code>
								</pre>
								<p>
									This is where I run into the first bug, which is that there is an inconsistency in my sample data. “Curry” is not sold by Hans’s Sausages but by another restaurant in the data. I will need to implement consistency checks in future development steps of this database. However, I am not sure yet how this will work, so I end this ePortfolio post here. I have learned that the database design my partner and I developed for our group project works well. The tables and relations perform as expected and all information is readily accessible. However, as this is the first project I have implemented in SQLite, I have not been able to ensure data consistency yet, which I will work on over the upcoming weeks. Another feature I was not able to implement in this prototype database was table encryption because SQLite does not offer table-by-table encryption. In the finalized database implementation that was a feature my group partner and I wanted to implement for the PaymentInfo table.
								</p>

								

								<p>
									<b>Sources</b>
									<ul>
										<li>
											Pandas (2023) pandas.DataFrame.to_sql. Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html [Accessed 04 March 2023].
										</li>
										<li>
											QuickDBD (2023) QuickDBD. Available from: https://app.quickdatabasediagrams.com/ [Accessed 04 March 2023].
										</li>
										<li>
											SQLiteTutorials (2022) SQLite Inner Join. Available from: https://www.sqlitetutorial.net/sqlite-inner-join/ [Accessed 04 March 2023].
										</li>
									</ul>
								</p>





<!-- Entry 3--><br>
								<p>
									<h3>API Security Requirements</h3>
									(13 March 2023)
								</p>	
								<p>
									APIs allow for the seamless integration of various systems (Cooksey, 2014) and thereby drive much of our modern, interconnected world. According to the Open Worldwide Application Security Project (OWASP), APIs have become an increasing target for attackers because they can be used to expose a wide range of sensitive information (OWASP, 2019). Among other things, APIs can be abused to expose user authentication information, lead to data leaks, can be used for denial of service attacks (DoS) against a server, or function as an entry point for injection attacks (ibid.).
								</p>
								<p>
									Due to their nature, securing an API is more complicated than traditional web security. Castle-and-Moat approaches that establish a clear perimeter around sensitive systems do not work (Kovacic, 2022). APIs can provide numerous access points on different networking ports, each with its own protocol. Conventional authentication mechanisms, therefore, are insufficient (ibid.). Further, classic web application firewalls are too cumbersome to adapt to rapidly changing API functionalities and since clients often use various interfaces for interacting with an API, it becomes increasingly difficult to distinguish between legitimate traffic versus bots and other malicious sources (ibid.). To properly secure APIs, OWASP has released a guideline of the top ten measures that organizations must take to secure their APIs (OWASP, 2019).
								</p>
								<p>
									In this ePortfolio post, I will briefly discuss five of these measures in the context of the API offered by the Deutscher Wetterdienst (DWD), which is the German Weather Service (DWD, 2019). This is a JSON-API that allows the public to query current weather data and severe weather warnings (ibid.) from the meteorological agency of the Federal Government of Germany.
								</p>
								<p>
									The first and most obvious OWASP API security recommendation is to “Implement a proper authorization mechanism” (OWASP, 2019: 9) and to use user policies to give various levels of access to different user groups (ibid.). In the case of the DWD, the first point is of little consequence as the public-facing API is free to use without the need for authorization (DWD, 2023). The second part is more relevant because users of the API must only extract certain pieces of meteorological information from the server but not, for example, send commands that would lead to code execution on the DWD’s servers. SQL injection attacks, for example, would need to be prevented with strictly enforced user roles.
								</p>
								<p>
									Another OWASP recommendation concerns the excessive exposure of information. Among other things, organizations should not “rely on the client side to filter sensitive data” and should classify sensitive or personally identifiable data (OWASP, 2019: 13). As the DWD does not, presumably, store personally identifiable data in its meteorological database, the latter is not of concern. Nonetheless, the API should not expose more information than what is strictly intended for the public. Thus, sensitive data should be flagged so a review mechanism is triggered every time someone queries this information.
								</p>
								<p>
									Rate limits are likely a pressing concern for a public API such as the DWD API. To avoid being purposely or accidentally DoSed by users, OWASP would be recommended to “implement a limit on how often a client can call the API within a defined timeframe” (OWASP, 2019: 15). As anyone can query the DWD API, strict rate limits are essential to keep its servers running and able to process all user requests in a timely fashion. 
								</p>
								<p>
									Fourth, to avoid security misconfiguration, OWASP recommends implementing a process to harden, review, and update configurations across the entire API life cycle (OWASP, 2019). This is essential for public services such as the DWD API. unfortunately, the DWD does not provide any technical documentation for its API besides a number of sample queries. Thus, I am unable to find out which security measures the DWD currently has in place.
								</p>
								<p>
									Finally, to prevent injection attacks, OWASP urges organizations to “validate, filter, and sanitize all client-provided data” (OWASP, 2019: 23). An injection attack involves injecting malicious code into a query or request in order to remotely execute it on a server (IBM, 2021). By validating, escaping, or filtering user inputs (in this case API calls), one prevents unwanted code from running on the DWD’s servers. As the DWD is a federal government agency, it is likely a target for a wide range of malicious activity, so proper API call sanitation should be implemented.
								</p>
								<p>
									The previous paragraphs highlighted five out of the ten OWASP API security recommendations. It has become clear that APIs offer a unique challenge to security professionals because they enable users to send commands to an organization’s servers. Public APIs, such as the one provided by the DWD likely attract special attention because they can easily and anonymously be used to test various attack vectors. Thus, the DWD must master the rare challenge of allowing virtually anyone to send commands to their servers but also maintain a tight net of security.
								</p>
								

								<p>
									<b>Sources</b>
									<ul>
										<li>
											Cooksey, B. (2014) <i>Real-Time Communication. An Introduction to APIs</i>. Zapier.
										</li>
										<li>
											DWD (2019) API des Deutschen Wetterdienstes. Available from: https://listed.to/@DieSieben/7851/api-des-deutschen-wetterdienstes [Accessed 13 March 2023].
										</li>
										<li>
											DWD (2023) Deutscher Wetterdienst: API. Available from: https://dwd.api.bund.dev/ [Accessed 13 March 2023].
										</li>
										<li>
											IBM (2021) Injection Attacks. Available from: https://www.ibm.com/docs/en/snips/4.6.0?topic=categories-injection-attacks [Accessed 13 March 2023].
										</li>
										<li>
											Kovacic, D. (2022) API Security: The Complete Guide to Threats, Methods & Tools. Available from: https://brightsec.com/blog/api-security/#api-and-general-app-security-difference [Accessed 13 March 2023].
										</li>
										<li>
											OWASP (2019) OWASP API Security Project. Available from: https://owasp.org/www-project-api-security/ [Accessed 13 March 2023].
										</li>
									</ul>
								</p>





<!-- Entry 4--><br>
								<p>
									<h3>Module Wiki Activity: Grandfather-Father-Son Backup Procedure</h3>
									(18 March 2023)
								</p>	
								<p>
									In its traditional form, the Grandfather-Father-Son (GFS) backup rotation scheme makes full daily backups that are kept for three days each (Burnett, Freidel & Friedman, 2009). This way, three backup “generations” emerge – the son, the father, and the grandfather. Modernized versions of the GFS work slightly differently: The grandfather is a full but infrequent (e.g. monthly) backup that is stored off-site. The father is a more frequent (e.g. weekly) full backup, and the son is a daily differential backup (ibid.).
								</p>
								<p>
									The GFS’s benefit is that organizations always have a fallback option in case of accidental data loss (Rivas, 2020). The downside is that the GFS is often insufficient to protect against intentional attacks. Ransomware operators, for example, have learned to target organizations’ backups (Posey, 2022). I personally have often seen hackers maintain a presence in an organization’s network for several weeks to ensure that all backups are corrupted. The GFS must, therefore, be supplemented with integrity checks for backups and off-site backups should occur relatively frequently. Vitally, the grandfather (and ideally father) backups must be stored entirely offline (Miller, 2023).
								</p>
								

								<p>
									<b>Sources</b>
									<ul>
										<li>
											Burnett, R., Friedel, A, & Friedman, M. (2009) The Bad Economy: Why You Need More IT Security Now. <i>Corporate Accounting & Finance</i> 20(5): 41-48. DOI: https://doi.org/10.1002/jcaf.20512
										</li>
										<li>
											Miller, M. (2023) Ransomware & The Importance Of Offline Backups. Available from: https://www.triaxiomsecurity.com/ransomware-the-importance-of-offline-backups/ [Accessed 18 March 2023].
										</li>
										<li>
											Posey, B. (2022) How To Protect Your Backups Against Ransomware Attacks. Available from: https://techgenix.com/ransomware-attacks-backup-guide/ [Accessed 18 March 2023].
										</li>
										<li>
											Rivas, K. (2020) Better Backup Practices: What Is the Grandfather-Father-Son Approach? Available from: https://www.backblaze.com/blog/better-backup-practices-what-is-the-grandfather-father-son-approach/ [Accessed 18 March 2023].
										</li>
									</ul>
								</p>




<!-- Entry 4--><br>
								<p>
									<h3>Second Peer Discussion – Summary Post</h3>
									(31 March 2023))
								</p>	
								<p>
									This collaborative discussion compared different compliance frameworks. My initial post contrasted the EU's (EU, 2016) General Data Protection Regulation (GDPR) with the ISO27001 standard and the German IT-Grundschutz. The GDPR is a legally binding framework designed to protect individual privacy and applies to all organizations that collect or process personally identifiable data in the EU (Tankard, 2016). In contrast, the ISO27001 and IT-Grundschutz are non-binding standards for protecting an organization's information security (BSI, 2023).
								</p>
								<p>
									Due to very limited participation from other students, no lively discussion ensued over the past weeks. One fellow student, however, asked about shortcomings in ISO27001. After evaluating my professional experience and research on the subject, I concluded that ISO27001 does not primarily suffer from technical shortfalls. Organizational agenda-setting is often more problematic. For example, executives often see little value in information security (Biscoe, 2023) and it is difficult to get all necessary departments involved in security efforts (Wailes-Fairbairn, 2019).
								</p>
								<p>
									The rest of the discussion focused on the UK's Guide to Data Protection (ICO, 2018). This guide outlines the data protection rules for UK organizations and the UK GDPR, which is established in the Data Protection Act 2018 (UK Public General Acts, 2018). As the name suggests, this is another binding legal framework serving the same general purpose as the EU's GDPR.
								</p>
								<p>
									Evaluating the value of the GDPR (both the EU's and UK's versions) against the ISO27001 and IT-Grundschutz standards, I argue that both frameworks make an important but very different contribution to data security. Binding legal frameworks force companies to reflect upon their data collection, processing, and storage. Companies that comply with the GDPR have a competitive advantage over others (Li, Yu & He, 2019). This benefits consumers, who otherwise have little power to control how their data is used.
								</p>
								<p>
									Non-binding security frameworks, on the other hand, help organizations to safeguard their data. This security goes beyond personal information and includes trade secrets, research, etc. Such information is vital for any company's economic future. In fact, it can be seen as a standard obligation for companies (Hsu, Wang & Lu, 2016). 
								</p>
								<p>
									In summary, the GDPR mostly protects consumers and individuals, while ISO27001 and the IT-Grundschutz help protect organizations. Both frameworks, thus, contribute to the overall goal of data protection - they just do so in very different ways. 
								</p>
								

								<p>
									<b>Sources</b>
									<ul>
										<li>
											Biscoe, C. (2023) ISO 27001 Implementation Challenges – And How to Overcome Them. Available from: https://www.itgovernanceusa.com/blog/3-iso-27001-implementation-challenges-and-how-to-overcome-them [Accessed 31 March 2023].
										</li>
										<li>
											BSI (2023) IT-Grundschutz. informationssicherheit mit System. Available from: https://www.bsi.bund.de/DE/Themen/Unternehmen-und-Organisationen/Standards-und-Zertifizierung/IT-Grundschutz/it-grundschutz_node.html [Accessed 31 March 2023].
										</li>
										<li>
											Hsu, C., Wang, T. & Lu, A. (2016) 'The Impact of ISO 27001 Certification on Firm Performance',  <i>49th Hawaii International Conference on System Sciences</i>. Koloha (USA), 5-8 January. IEEE. 4842-4848.
										</li>
										<li>
											ICO (2018) Guide to Data protection. Available from: https://ico.org.uk/for-organisations/guide-to-data-protection/ [Accessed 31 March 2023].
										</li>
										<li>
											Li, H., Lu, L. & He, W. (2019) The Impact of GDPR on Global Technology Development. <i>Journal of Global Information Technology Management</i> 22(1): 1-6. DOI: https://doi.org/10.1080/1097198X.2019.1569186
										</li>
										<li>
											Tankard, C. (2016) What the GDPR means for businesses. <i>Network Security</i> 2016(6): 5-8. DOI: https://doi.org/10.1016/S1353-4858(16)30056-3
										</li>
										<li>
											UK Public General Acts (2018) Data Protection Act 2018, Chapter 2. Available from: https://www.legislation.gov.uk/ukpga/2018/12/contents/enacted [Accessed 31 March 2023].
										</li>
										<li>
											Wailes-Fairbairn, J. (2019) ISO 27001 – Top 5 challenges to becoming certified. Available from: https://www.srm-solutions.com/blog/iso-27001-top-5-challenges-to-becoming-certified/ [Accessed 31 March 2023].
										</li>
									</ul>
								</p>




			<!-- Footer -->
				<footer id="footer">
					<ul class="icons">
						<li><a href="https://twitter.com/RealJohnGeiger" class="icon brands alt fa-twitter"><span class="label">Twitter</span></a></li>
						<li><a href="https://de.linkedin.com/in/johannes-geiger?trk=people-guest_people_search-card" class="icon brands alt fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
						<li><a href="https://www.instagram.com/realjohngeiger/" class="icon brands alt fa-instagram"><span class="label">Instagram</span></a></li>
						<li><a href="https://github.com/johnny80y/" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
					</ul>
					<ul class="copyright">
						<li>&copy; Untitled. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>
				</footer>

		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.dropotron.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
